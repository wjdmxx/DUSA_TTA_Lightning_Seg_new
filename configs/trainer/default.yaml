# PyTorch Lightning Trainer configuration
# Uses FSDP strategy for dual-GPU model parallelism

# Number of GPUs to use
devices: 2

# FSDP strategy for model parallelism across 2 GPUs
# Usage: CUDA_VISIBLE_DEVICES=0,1 python scripts/run_tta.py
# This will distribute VAE + SegFormer on GPU 0, SD3 Transformer across GPUs

# Precision: bf16 mixed precision for memory efficiency
# Loss computation is forced to float32 to avoid overflow
precision: "bf16-mixed"

# Trainer settings
accelerator: "gpu"
enable_checkpointing: false
enable_model_summary: true
